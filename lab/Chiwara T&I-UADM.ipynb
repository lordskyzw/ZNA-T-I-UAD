{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>plan</th>\n",
       "      <th>session_date</th>\n",
       "      <th>mb_used</th>\n",
       "      <th>total_messages_used</th>\n",
       "      <th>total_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>ultimate</td>\n",
       "      <td>2018-12-26</td>\n",
       "      <td>270.99</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>ultimate</td>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>880.22</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>ultimate</td>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>660.40</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>ultimate</td>\n",
       "      <td>2018-12-29</td>\n",
       "      <td>89.86</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>ultimate</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id      plan session_date  mb_used  total_messages_used  total_minutes\n",
       "0     1000  ultimate   2018-12-26   270.99                 11.0           16.0\n",
       "1     1000  ultimate   2018-12-27   880.22                 11.0           16.0\n",
       "2     1000  ultimate   2018-12-28   660.40                 11.0           16.0\n",
       "3     1000  ultimate   2018-12-29    89.86                 11.0           16.0\n",
       "4     1000  ultimate   2018-12-31     0.00                 11.0           16.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "\n",
    "data = pd.read_csv('merged_dataset2.csv')\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SEPARATING DATASETS IN ORDER TO FIND ANOMALIES IN EACH DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('merged_dataset2.csv')\n",
    "\n",
    "# Filter the DataFrame for 'ultimate' plans\n",
    "ultimate_df = df[df['plan'] == 'ultimate']\n",
    "\n",
    "# Filter the DataFrame for 'surf' plans\n",
    "surf_df = df[df['plan'] == 'surf']\n",
    "\n",
    "# Save the DataFrames to new CSV files\n",
    "ultimate_df.to_csv('ultimate_plans.csv', index=False)\n",
    "surf_df.to_csv('surf_plans.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATING SURF PLANS FINAL CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('cleandatasets/surf_plans.csv')\n",
    "\n",
    "\n",
    "# Compute the running totals\n",
    "df['total_mb_used'] = df['mb_used'].cumsum()\n",
    "df['total_messages_used'] = df['total_messages_used'].cumsum()\n",
    "df['total_minutes'] = df['total_minutes'].cumsum()\n",
    "\n",
    "# Drop the 'session_date' column as it's no longer needed\n",
    "df = df.drop(columns=['session_date'])\n",
    "\n",
    "# Group by 'user_id' and aggregate the data\n",
    "# For 'mb_used', we will take the last value in the accumulated data as the total usage\n",
    "# For 'total_messages_used' and 'total_minutes', we will also take the last accumulated values\n",
    "aggregated_df = df.groupby('user_id').agg({\n",
    "    'mb_used': 'last',\n",
    "    'total_messages_used': 'last',\n",
    "    'total_minutes': 'last',\n",
    "    'total_mb_used': 'last'\n",
    "}).reset_index()\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "aggregated_df.dropna(inplace=True)\n",
    "aggregated_df.drop(columns=['mb_used'], inplace=True)\n",
    "aggregated_df.to_csv('cleandatasets/surf_plans_final.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING A FINAL CSV FILE FOR ULTIMATE PLANS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('cleandatasets/ultimate_plans.csv')\n",
    "\n",
    "\n",
    "# Compute the running totals\n",
    "df['total_mb_used'] = df['mb_used'].cumsum()\n",
    "df['total_messages_used'] = df['total_messages_used'].cumsum()\n",
    "df['total_minutes'] = df['total_minutes'].cumsum()\n",
    "\n",
    "# Drop the 'session_date' column as it's no longer needed\n",
    "df = df.drop(columns=['session_date'])\n",
    "\n",
    "# Group by 'user_id' and aggregate the data\n",
    "# For 'mb_used', we will take the last value in the accumulated data as the total usage\n",
    "# For 'total_messages_used' and 'total_minutes', we will also take the last accumulated values\n",
    "aggregated_df = df.groupby('user_id').agg({\n",
    "    'mb_used': 'last',\n",
    "    'total_messages_used': 'last',\n",
    "    'total_minutes': 'last',\n",
    "    'total_mb_used': 'last'\n",
    "}).reset_index()\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "aggregated_df.dropna(inplace=True)\n",
    "aggregated_df.drop(columns=['mb_used'], inplace=True)\n",
    "aggregated_df.to_csv('cleandatasets/ultimate_plans_final.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FINDING ANOMALIES IN THE ULTIMATE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   user_id  total_messages_used  total_minutes  total_mb_used\n",
      "0     1000                 55.0           80.0        1901.47\n",
      "1     1006              10835.0          850.0       36088.66\n",
      "2     1008              25619.0        39394.0       91561.70\n",
      "3     1011             177729.0       208724.0      223340.30\n",
      "4     1013             178801.0       211069.0      243454.22\n",
      "Shape of scaled data: (131, 3)\n",
      "Anomalies Detected:\n",
      "     user_id  total_messages_used  total_minutes  total_mb_used  Anomaly\n",
      "0       1000                 55.0           80.0        1901.47       -1\n",
      "1       1006              10835.0          850.0       36088.66       -1\n",
      "2       1008              25619.0        39394.0       91561.70       -1\n",
      "3       1011             177729.0       208724.0      223340.30       -1\n",
      "4       1013             178801.0       211069.0      243454.22       -1\n",
      "5       1026             179681.0       213349.0      256678.68       -1\n",
      "14      1039            1006917.0       838780.0     1219460.65       -1\n",
      "17      1057            1315084.0      1579620.0     1880267.67       -1\n",
      "18      1059            1522421.0      1903657.0     2001087.87       -1\n",
      "122     1465            7555699.0     12586208.0    11637675.12       -1\n",
      "123     1467            7796773.0     12823196.0    11789844.79       -1\n",
      "124     1475            7906393.0     12889316.0    11880739.83       -1\n",
      "130     1497            7976089.0     13249462.0    12394583.78       -1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('cleandatasets/ultimate_plans_final.csv')\n",
    "\n",
    "# Print the first few rows to understand the structure of the dataset\n",
    "print(\"Original DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Identify numerical columns\n",
    "numerical_cols = ['total_messages_used', 'total_minutes', 'total_mb_used']\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the numerical columns\n",
    "scaled_data = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Display the shape of scaled data to ensure correct scaling\n",
    "print(f\"Shape of scaled data: {scaled_data.shape}\")\n",
    "\n",
    "# Train Isolation Forest model\n",
    "model = IsolationForest(contamination=0.1, random_state=42)\n",
    "model.fit(scaled_data)\n",
    "\n",
    "# Predict anomalies\n",
    "anomalies = model.predict(scaled_data)\n",
    "\n",
    "# Add anomalies to the original data\n",
    "df['Anomaly'] = anomalies\n",
    "\n",
    "# Filter the data to show only the anomalies\n",
    "anomalies_data = df[df['Anomaly'] == -1]\n",
    "\n",
    "# Print the anomalies\n",
    "print(\"Anomalies Detected:\")\n",
    "print(anomalies_data)\n",
    "\n",
    "# Save the anomalies to a new CSV file\n",
    "anomalies_data.to_csv('cleandatasets/ultimate_plans_anomalies.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FINDING ANOMALIES IN THE SURF DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   user_id  total_messages_used  total_minutes  total_mb_used\n",
      "0     1001              50715.0        63945.0       80437.94\n",
      "1     1002              61627.0        77957.0      120731.27\n",
      "2     1003              64227.0        85705.0      147775.41\n",
      "3     1004             145647.0       255905.0      304128.22\n",
      "4     1005             146307.0       259445.0      321268.39\n",
      "Shape of scaled data: (262, 3)\n",
      "Anomalies Detected:\n",
      "     user_id  total_messages_used  total_minutes  total_mb_used  Anomaly\n",
      "0       1001              50715.0        63945.0       80437.94       -1\n",
      "1       1002              61627.0        77957.0      120731.27       -1\n",
      "2       1003              64227.0        85705.0      147775.41       -1\n",
      "3       1004             145647.0       255905.0      304128.22       -1\n",
      "4       1005             146307.0       259445.0      321268.39       -1\n",
      "5       1007             215922.0       363185.0      465121.13       -1\n",
      "6       1014             217382.0       990286.0      824481.08       -1\n",
      "8       1016             230864.0      1016902.0      901335.45       -1\n",
      "20      1046             480609.0      1959161.0     1834346.59       -1\n",
      "22      1052             502761.0      2083443.0     1993333.99       -1\n",
      "23      1053             525336.0      2172539.0     2090466.24       -1\n",
      "24      1054             654978.0      2336963.0     2262114.89       -1\n",
      "25      1055             860482.0      2426195.0     2375700.57       -1\n",
      "29      1064            1041765.0      2789271.0     2716339.25       -1\n",
      "30      1065            1062565.0      2845015.0     2780564.35       -1\n",
      "31      1066            1259797.0      3195454.0     2975044.55       -1\n",
      "33      1070            1278894.0      3321037.0     3164818.96       -1\n",
      "34      1072            1310008.0      3641999.0     3441974.51       -1\n",
      "171     1323            8076250.0     19804577.0    17599941.87       -1\n",
      "195     1361           10147416.0     23051125.0    20120075.88       -1\n",
      "217     1404           11410727.0     25770403.0    22433905.14       -1\n",
      "233     1439           12271393.0     27432044.0    23984323.19       -1\n",
      "257     1489           12857698.0     28615963.0    25388881.15       -1\n",
      "258     1491           12939089.0     28707901.0    25451931.82       -1\n",
      "259     1492           12951509.0     28726761.0    25492755.78       -1\n",
      "260     1494           13000925.0     28846041.0    25584144.97       -1\n",
      "261     1496           13015550.0     28963286.0    25747304.57       -1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('cleandatasets/surf_plans_final.csv')\n",
    "\n",
    "# Print the first few rows to understand the structure of the dataset\n",
    "print(\"Original DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Identify numerical columns\n",
    "numerical_cols = ['total_messages_used', 'total_minutes', 'total_mb_used']\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the numerical columns\n",
    "scaled_data = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Display the shape of scaled data to ensure correct scaling\n",
    "print(f\"Shape of scaled data: {scaled_data.shape}\")\n",
    "\n",
    "# Train Isolation Forest model\n",
    "model = IsolationForest(contamination=0.1, random_state=42)\n",
    "model.fit(scaled_data)\n",
    "\n",
    "# Predict anomalies\n",
    "anomalies = model.predict(scaled_data)\n",
    "\n",
    "# Add anomalies to the original data\n",
    "df['Anomaly'] = anomalies\n",
    "\n",
    "# Filter the data to show only the anomalies\n",
    "anomalies_data = df[df['Anomaly'] == -1]\n",
    "\n",
    "# Print the anomalies\n",
    "print(\"Anomalies Detected:\")\n",
    "print(anomalies_data)\n",
    "\n",
    "# Save the anomalies to a new CSV file\n",
    "anomalies_data.to_csv('cleandatasets/surf_plans_anomalies.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>session_date</th>\n",
       "      <th>mb_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>2018-12-26</td>\n",
       "      <td>270.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>880.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>660.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>2018-12-29</td>\n",
       "      <td>89.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id session_date  mb_used\n",
       "0     1000   2018-12-26   270.99\n",
       "1     1000   2018-12-27   880.22\n",
       "2     1000   2018-12-28   660.40\n",
       "3     1000   2018-12-29    89.86\n",
       "4     1000   2018-12-31     0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "df2 = df.drop(columns=['plan','total_messages_used', 'total_minutes'])\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_data shape: (7032, 27)\n",
      "scaled_data shape: (7032, 4)\n",
      "Dimensions of the first element in the tuple: 2\n",
      "Dimensions of the second element in the tuple: 2\n",
      "Error: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)\n",
      "encoded_data:   (0, 1)\t1.0\n",
      "  (0, 4)\t1.0\n",
      "  (0, 11)\t1.0\n",
      "  (0, 22)\t1.0\n",
      "  (0, 24)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (1, 3)\t1.0\n",
      "  (1, 9)\t1.0\n",
      "  (1, 13)\t1.0\n",
      "  (1, 20)\t1.0\n",
      "  (1, 25)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (2, 3)\t1.0\n",
      "  (2, 9)\t1.0\n",
      "  (2, 11)\t1.0\n",
      "  (2, 22)\t1.0\n",
      "  (2, 25)\t1.0\n",
      "  (2, 26)\t1.0\n",
      "  (3, 0)\t1.0\n",
      "  (3, 4)\t1.0\n",
      "  (3, 9)\t1.0\n",
      "  (3, 13)\t1.0\n",
      "  (3, 15)\t1.0\n",
      "  (3, 20)\t1.0\n",
      "  (4, 3)\t1.0\n",
      "  (4, 6)\t1.0\n",
      "  (4, 22)\t1.0\n",
      "  (4, 24)\t1.0\n",
      "  (4, 26)\t1.0\n",
      "scaled_data: [[-0.44032709 -1.28024804 -1.16169394 -0.99419409]\n",
      " [-0.44032709  0.06430269 -0.26087792 -0.17373982]\n",
      " [-0.44032709 -1.23950408 -0.36392329 -0.95964911]\n",
      " [-0.44032709  0.51248626 -0.74785042 -0.19524771]\n",
      " [-0.44032709 -1.23950408  0.19617818 -0.94045745]]\n"
     ]
    }
   ],
   "source": [
    "# Ensure both encoded_data and scaled_data have the correct dimensions\n",
    "print(f'encoded_data shape: {encoded_data.shape}')\n",
    "print(f'scaled_data shape: {scaled_data.shape}')\n",
    "\n",
    "# Check the dimensions of each element in the tuple\n",
    "print(f'Dimensions of the first element in the tuple: {encoded_data.ndim}')\n",
    "print(f'Dimensions of the second element in the tuple: {scaled_data.ndim}')\n",
    "\n",
    "# Combining encoded categorical data with scaled numerical data\n",
    "try:\n",
    "    processed_data = np.hstack((encoded_data, scaled_data))\n",
    "    print(f'processed_data shape: {processed_data.shape}')\n",
    "except ValueError as e:\n",
    "    print(f'Error: {e}')\n",
    "    print(f'encoded_data: {encoded_data[:5]}')\n",
    "    print(f'scaled_data: {scaled_data[:5]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Combine encoded categorical data with scaled numerical data\n",
    "processed_data = np.hstack((encoded_data, scaled_data))\n",
    "\n",
    "# Display the shape of processed data to ensure the combination is correct\n",
    "processed_data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### after facing the previous error, i had to revise the work and i found that i was using the wrong approach to solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The entire workflow as a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
      "27      Male              0     Yes        Yes       1           No   \n",
      "54    Female              1     Yes        Yes      60          Yes   \n",
      "62      Male              0     Yes         No      72           No   \n",
      "71    Female              0     Yes        Yes      52          Yes   \n",
      "79    Female              0     Yes        Yes      45          Yes   \n",
      "...      ...            ...     ...        ...     ...          ...   \n",
      "6996  Female              0     Yes        Yes      41          Yes   \n",
      "7000  Female              0      No         No      67          Yes   \n",
      "7007    Male              1     Yes         No      72           No   \n",
      "7031    Male              1     Yes         No      55          Yes   \n",
      "7036  Female              0      No         No      12           No   \n",
      "\n",
      "         MultipleLines InternetService       OnlineSecurity  \\\n",
      "27    No phone service             DSL                   No   \n",
      "54                  No             DSL                  Yes   \n",
      "62    No phone service             DSL                  Yes   \n",
      "71                  No              No  No internet service   \n",
      "79                 Yes              No  No internet service   \n",
      "...                ...             ...                  ...   \n",
      "6996                No             DSL                   No   \n",
      "7000                No              No  No internet service   \n",
      "7007  No phone service             DSL                  Yes   \n",
      "7031               Yes             DSL                  Yes   \n",
      "7036  No phone service             DSL                   No   \n",
      "\n",
      "             OnlineBackup  ...          TechSupport          StreamingTV  \\\n",
      "27                    Yes  ...                   No                   No   \n",
      "54                    Yes  ...                  Yes                   No   \n",
      "62                    Yes  ...                   No                   No   \n",
      "71    No internet service  ...  No internet service  No internet service   \n",
      "79    No internet service  ...  No internet service  No internet service   \n",
      "...                   ...  ...                  ...                  ...   \n",
      "6996                  Yes  ...                  Yes                   No   \n",
      "7000  No internet service  ...  No internet service  No internet service   \n",
      "7007                  Yes  ...                  Yes                  Yes   \n",
      "7031                  Yes  ...                   No                   No   \n",
      "7036                  Yes  ...                  Yes                  Yes   \n",
      "\n",
      "          StreamingMovies        Contract PaperlessBilling  \\\n",
      "27                     No  Month-to-month               No   \n",
      "54                    Yes        One year              Yes   \n",
      "62                     No        Two year               No   \n",
      "71    No internet service        One year              Yes   \n",
      "79    No internet service        One year              Yes   \n",
      "...                   ...             ...              ...   \n",
      "6996                  Yes        One year              Yes   \n",
      "7000  No internet service        One year               No   \n",
      "7007                  Yes        Two year              Yes   \n",
      "7031                   No        One year               No   \n",
      "7036                  Yes        One year               No   \n",
      "\n",
      "                  PaymentMethod MonthlyCharges  TotalCharges  Churn Anomaly  \n",
      "27             Electronic check          30.20         30.20    Yes      -1  \n",
      "54      Credit card (automatic)          74.85       4456.35     No      -1  \n",
      "62    Bank transfer (automatic)          42.10       2962.00     No      -1  \n",
      "71             Electronic check          20.40       1090.65     No      -1  \n",
      "79      Credit card (automatic)          25.90       1216.60     No      -1  \n",
      "...                         ...            ...           ...    ...     ...  \n",
      "6996  Bank transfer (automatic)          66.50       2728.60    Yes      -1  \n",
      "7000           Electronic check          20.55       1343.40     No      -1  \n",
      "7007  Bank transfer (automatic)          63.10       4685.55     No      -1  \n",
      "7031    Credit card (automatic)          60.00       3316.10     No      -1  \n",
      "7036           Electronic check          60.65        743.30     No      -1  \n",
      "\n",
      "[704 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Cell 2: Load the dataset\n",
    "data = pd.read_csv('usage.csv')\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "#data.head()\n",
    "\n",
    "# Cell 3: Drop irrelevant columns\n",
    "data.drop(columns=['customerID'], inplace=True)\n",
    "\n",
    "# Cell 4: Handle missing values and convert numerical columns to appropriate type\n",
    "data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')  # Convert TotalCharges to numeric, setting errors to NaN\n",
    "data.dropna(inplace=True)  # Drop rows with NaN values\n",
    "\n",
    "# Display the first few rows to ensure changes\n",
    "#data.head()\n",
    "\n",
    "# Cell 5: Identify categorical columns\n",
    "categorical_cols = ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService',\n",
    "                    'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', \n",
    "                    'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'Churn']\n",
    "\n",
    "# Cell 6: Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "\n",
    "# Cell 7: Fit and transform the categorical columns\n",
    "encoded_data = encoder.fit_transform(data[categorical_cols])\n",
    "\n",
    "# Display the shape of encoded data to understand the number of features created\n",
    "encoded_data.shape\n",
    "\n",
    "# Cell 8: Identify numerical columns\n",
    "numerical_cols = ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "\n",
    "# Cell 9: Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Cell 10: Fit and transform the numerical columns\n",
    "scaled_data = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "# Display the shape of scaled data to ensure correct scaling\n",
    "scaled_data.shape\n",
    "\n",
    "# # Ensure both encoded_data and scaled_data have the correct dimensions\n",
    "# print(f'encoded_data shape: {encoded_data.shape}')\n",
    "# print(f'scaled_data shape: {scaled_data.shape}')\n",
    "\n",
    "# # Check the dimensions of each element in the tuple\n",
    "# print(f'Dimensions of the first element in the tuple: {encoded_data.ndim}')\n",
    "# print(f'Dimensions of the second element in the tuple: {scaled_data.ndim}')\n",
    "\n",
    "# Cell 11: Combine encoded categorical data with scaled numerical data\n",
    "try:\n",
    "    processed_data = np.hstack((encoded_data, scaled_data))\n",
    "    #print(f'processed_data shape: {processed_data.shape}')\n",
    "except ValueError as e:\n",
    "    print(f'Error: {e}')\n",
    "    print(f'encoded_data: {encoded_data[:5]}')\n",
    "    print(f'scaled_data: {scaled_data[:5]}')\n",
    "\n",
    "# Continue with the Isolation Forest model training and prediction\n",
    "# Cell 12: Train Isolation Forest model\n",
    "model = IsolationForest(contamination=0.1, random_state=42)\n",
    "model.fit(processed_data)\n",
    "\n",
    "# Cell 13: Predict anomalies\n",
    "anomalies = model.predict(processed_data)\n",
    "\n",
    "# Cell 14: Add anomalies to the original data\n",
    "data['Anomaly'] = anomalies\n",
    "\n",
    "# Cell 15: Display the anomalies\n",
    "anomalies_data = data[data['Anomaly'] == -1]\n",
    "print(anomalies_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the anomalies to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "anomalies_data.to_csv('anomalies.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR MESSAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  user_id message_date  total_messages_used\n",
      "0  1000_125     1000   2018-12-27                   11\n",
      "1  1000_160     1000   2018-12-31                   11\n",
      "2  1000_223     1000   2018-12-31                   11\n",
      "3  1000_251     1000   2018-12-27                   11\n",
      "4  1000_255     1000   2018-12-26                   11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('megaline_messages.csv')\n",
    "\n",
    "# Calculate the total messages per user\n",
    "user_message_counts = df['user_id'].value_counts().reset_index()\n",
    "user_message_counts.columns = ['user_id', 'total_messages_used']\n",
    "\n",
    "# Merge the total messages information back into the original DataFrame\n",
    "df = df.merge(user_message_counts, on='user_id')\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     user_id  total_messages_used\n",
      "0       1000                   11\n",
      "11      1001                  207\n",
      "218     1002                   88\n",
      "306     1003                   50\n",
      "356     1004                  177\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('megaline_messages.csv')\n",
    "\n",
    "# Calculate the total messages per user\n",
    "user_message_counts = df['user_id'].value_counts().reset_index()\n",
    "user_message_counts.columns = ['user_id', 'total_messages_used']\n",
    "\n",
    "# Merge the total messages information back into the original DataFrame\n",
    "df = df.merge(user_message_counts, on='user_id')\n",
    "\n",
    "# Drop the 'id' column\n",
    "df = df.drop(columns=['id', 'message_date'])\n",
    "\n",
    "# Drop duplicates to ensure one row per user_id\n",
    "df = df.drop_duplicates(subset=['user_id'])\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "df.to_csv('cleaned_messages.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CALLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>call_date</th>\n",
       "      <th>duration</th>\n",
       "      <th>total_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000_93</td>\n",
       "      <td>1000</td>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>8.52</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1001_0</td>\n",
       "      <td>1001</td>\n",
       "      <td>2018-09-06</td>\n",
       "      <td>10.06</td>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>1002_0</td>\n",
       "      <td>1002</td>\n",
       "      <td>2018-11-14</td>\n",
       "      <td>12.32</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>1003_0</td>\n",
       "      <td>1003</td>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>1004_0</td>\n",
       "      <td>1004</td>\n",
       "      <td>2018-11-28</td>\n",
       "      <td>8.82</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  user_id   call_date  duration  total_minutes\n",
       "0    1000_93     1000  2018-12-27      8.52             16\n",
       "16    1001_0     1001  2018-09-06     10.06            261\n",
       "277   1002_0     1002  2018-11-14     12.32            113\n",
       "390   1003_0     1003  2018-12-28      0.00            149\n",
       "539   1004_0     1004  2018-11-28      8.82            370"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('megaline_calls.csv')\n",
    "\n",
    "# calculate the total minutes oer user\n",
    "\n",
    "minutes_count = df['user_id'].value_counts().reset_index()\n",
    "minutes_count.columns = ['user_id', 'total_minutes']\n",
    "\n",
    "df = df.merge(minutes_count, on= 'user_id')\n",
    "\n",
    "df = df.drop_duplicates(subset = ['user_id'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated DataFrame:\n",
      "     user_id   call_date  total_minutes\n",
      "0       1000  2018-12-27             16\n",
      "16      1001  2018-09-06            261\n",
      "277     1002  2018-11-14            113\n",
      "390     1003  2018-12-28            149\n",
      "539     1004  2018-11-28            370\n"
     ]
    }
   ],
   "source": [
    "# Check if 'duration' column exists, then drop it\n",
    "if 'duration' in df.columns:\n",
    "    df = df.drop(columns=['duration', 'id'])\n",
    "    print(\"\\nUpdated DataFrame:\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"\\n'duration' column not found in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>total_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1001</td>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>1002</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>1003</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>1004</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  total_minutes\n",
       "0       1000             16\n",
       "16      1001            261\n",
       "277     1002            113\n",
       "390     1003            149\n",
       "539     1004            370"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "df = df.drop(columns=['call_date'])\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_calls.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR THE INTERNET USAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id session_date  mb_used  total_mb_used\n",
      "0     1000   2018-12-29    89.86        1901.47\n",
      "1     1000   2018-12-31     0.00        1901.47\n",
      "2     1000   2018-12-28   660.40        1901.47\n",
      "3     1000   2018-12-26   270.99        1901.47\n",
      "4     1000   2018-12-27   880.22        1901.47\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('megaline_internet.csv')\n",
    "\n",
    "# Calculate the total mb_used per user\n",
    "user_mb_used_totals = df.groupby('user_id')['mb_used'].sum().reset_index()\n",
    "user_mb_used_totals.columns = ['user_id', 'total_mb_used']\n",
    "\n",
    "# Merge the total mb_used information back into the original DataFrame\n",
    "df = df.merge(user_mb_used_totals, on='user_id')\n",
    "\n",
    "# Drop the 'id'\n",
    "df = df.drop(columns=['id'])\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df.to_csv('trendusage.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the cleaned DataFrame to verify\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING USERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>plan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>ultimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>surf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>surf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003</td>\n",
       "      <td>surf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>surf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id      plan\n",
       "0     1000  ultimate\n",
       "1     1001      surf\n",
       "2     1002      surf\n",
       "3     1003      surf\n",
       "4     1004      surf"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('megaline_users.csv')\n",
    "\n",
    "df = df.drop(columns=['first_name', 'last_name', 'age', 'city', 'reg_date', 'churn_date'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_users.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
